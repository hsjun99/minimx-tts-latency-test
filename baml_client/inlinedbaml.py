# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "clients/providers.baml": "client<llm> Cerebras_Qwen3_32B {\n  provider \"openai-generic\"\n  options {\n    base_url \"https://api.cerebras.ai/v1\"\n    api_key env.CEREBRAS_API_KEY\n    model \"qwen-3-32b\" \n    temperature 0.0\n  }\n}\n\nclient<llm> Cerebras_GPT_OSS_120B_MEDIUM_REASONING {\n  provider \"openai-generic\"\n  options {\n    base_url \"https://api.cerebras.ai/v1\"\n    api_key env.CEREBRAS_API_KEY\n    model \"gpt-oss-120b\"\n    temperature 0.0\n    reasoning_effort \"medium\"\n  }\n}\n\nclient<llm> Cerebras_GPT_OSS_120B_LOW_REASONING {\n  provider \"openai-generic\"\n  options {\n    base_url \"https://api.cerebras.ai/v1\"\n    api_key env.CEREBRAS_API_KEY\n    model \"gpt-oss-120b\"\n    temperature 0.0\n    reasoning_effort \"low\"\n  }\n}\n\nclient<llm> Groq_Qwen3_32B {\n  provider \"openai-generic\"\n  options {\n    base_url \"https://api.groq.com/openai/v1\"\n    api_key env.GROQ_API_KEY\n    model \"qwen/qwen3-32b\" \n    temperature 0.0\n  }\n}\n\n\nclient<llm> Groq_GPT_OSS_120B_MEDIUM_REASONING {\n  provider openai-generic\n  options {\n    base_url \"https://api.groq.com/openai/v1\"\n    api_key env.GROQ_API_KEY\n    model \"openai/gpt-oss-120b\"\n    temperature 0.0\n    reasoning_effort \"medium\"\n  }\n}\n\nclient<llm> Groq_GPT_OSS_120B_LOW_REASONING {\n  provider openai-generic\n  options {\n    base_url \"https://api.groq.com/openai/v1\"\n    api_key env.GROQ_API_KEY\n    model \"openai/gpt-oss-120b\"\n    temperature 0.0\n    reasoning_effort \"low\"\n  }\n}",
    "functions/generate_queries.baml": "function GenerateMultiRAGQuery_Cerebras_Qwen3_32B(\n    conversation_history: ConversationMessage[],\n    user_query: string\n) -> MultiRAGQueryResult {\n    client Cerebras_Qwen3_32B\n    prompt #\"\n    You are a Multi-Query RAG Generation Agent. Analyze the full conversation context to generate 2-3 focused search queries addressing the user's underlying information needs.\n\n    KEY APPROACH:\n    1. Analyze the COMPLETE conversation thread to identify the main topic being discussed\n    2. Generate queries about the actual information need, not communication issues\n\n    PRINCIPLES:\n    - Generate all queries in Korean language (한국어)\n    - Focus on substantive topics from conversation analysis\n    - Avoid queries about communication problems - target the underlying subject matter\n\n    Context:\n    - User's Latest Query: {{ user_query }}\n    - Conversation History: {{ conversation_history }}\n\n    Generate 2-3 Korean search queries that address the user's actual information need based on the conversation context.\n\n    /no_think\n\n    Output Format:\n    {{ ctx.output_format }}\n    \"#\n}\n\nfunction GenerateMultiRAGQuery_Groq_Qwen3_32B(\n    conversation_history: ConversationMessage[],\n    user_query: string\n) -> MultiRAGQueryResult {\n    client Groq_Qwen3_32B\n    prompt #\"\n    You are a Multi-Query RAG Generation Agent. Analyze the full conversation context to generate 2-3 focused search queries addressing the user's underlying information needs.\n\n    KEY APPROACH:\n    1. Analyze the COMPLETE conversation thread to identify the main topic being discussed\n    2. Generate queries about the actual information need, not communication issues\n\n    PRINCIPLES:\n    - Generate all queries in Korean language (한국어)\n    - Focus on substantive topics from conversation analysis\n    - Avoid queries about communication problems - target the underlying subject matter\n\n    Context:\n    - User's Latest Query: {{ user_query }}\n    - Conversation History: {{ conversation_history }}\n\n    Generate 2-3 Korean search queries that address the user's actual information need based on the conversation context.\n\n    Output Format:\n    {{ ctx.output_format }}\n    \"#\n}\n\nfunction GenerateMultiRAGQuery_Cerebras_GPT_OSS_120B_Medium(\n    conversation_history: ConversationMessage[],\n    user_query: string\n) -> MultiRAGQueryResult {\n    client Cerebras_GPT_OSS_120B_MEDIUM_REASONING\n    prompt #\"\n    You are a Multi-Query RAG Generation Agent. Analyze the full conversation context to generate 2-3 focused search queries addressing the user's underlying information needs.\n\n    KEY APPROACH:\n    1. Analyze the COMPLETE conversation thread to identify the main topic being discussed\n    2. Generate queries about the actual information need, not communication issues\n\n    PRINCIPLES:\n    - Generate all queries in Korean language (한국어)\n    - Focus on substantive topics from conversation analysis\n    - Avoid queries about communication problems - target the underlying subject matter\n\n    Context:\n    - User's Latest Query: {{ user_query }}\n    - Conversation History: {{ conversation_history }}\n\n    Generate 2-3 Korean search queries that address the user's actual information need based on the conversation context.\n\n    /no_think\n\n    Output Format:\n    {{ ctx.output_format }}\n    \"#\n}\n\nfunction GenerateMultiRAGQuery_Groq_GPT_OSS_120B_Medium(\n    conversation_history: ConversationMessage[],\n    user_query: string\n) -> MultiRAGQueryResult {\n    client Groq_GPT_OSS_120B_MEDIUM_REASONING\n    prompt #\"\n    You are a Multi-Query RAG Generation Agent. Analyze the full conversation context to generate 2-3 focused search queries addressing the user's underlying information needs.\n\n    KEY APPROACH:\n    1. Analyze the COMPLETE conversation thread to identify the main topic being discussed\n    2. Generate queries about the actual information need, not communication issues\n\n    PRINCIPLES:\n    - Generate all queries in Korean language (한국어)\n    - Focus on substantive topics from conversation analysis\n    - Avoid queries about communication problems - target the underlying subject matter\n\n    Context:\n    - User's Latest Query: {{ user_query }}\n    - Conversation History: {{ conversation_history }}\n\n    Generate 2-3 Korean search queries that address the user's actual information need based on the conversation context.\n\n    Output Format:\n    {{ ctx.output_format }}\n    \"#\n}\n\nfunction GenerateMultiRAGQuery_Cerebras_GPT_OSS_120B_Low(\n    conversation_history: ConversationMessage[],\n    user_query: string\n) -> MultiRAGQueryResult {\n    client Cerebras_GPT_OSS_120B_LOW_REASONING\n    prompt #\"\n    You are a Multi-Query RAG Generation Agent. Analyze the full conversation context to generate 2-3 focused search queries addressing the user's underlying information needs.\n\n    KEY APPROACH:\n    1. Analyze the COMPLETE conversation thread to identify the main topic being discussed\n    2. Generate queries about the actual information need, not communication issues\n\n    PRINCIPLES:\n    - Generate all queries in Korean language (한국어)\n    - Focus on substantive topics from conversation analysis\n    - Avoid queries about communication problems - target the underlying subject matter\n\n    Context:\n    - User's Latest Query: {{ user_query }}\n    - Conversation History: {{ conversation_history }}\n\n    Generate 2-3 Korean search queries that address the user's actual information need based on the conversation context.\n\n    /no_think\n\n    Output Format:\n    {{ ctx.output_format }}\n    \"#\n}\n\nfunction GenerateMultiRAGQuery_Groq_GPT_OSS_120B_Low(\n    conversation_history: ConversationMessage[],\n    user_query: string\n) -> MultiRAGQueryResult {\n    client Groq_GPT_OSS_120B_LOW_REASONING\n    prompt #\"\n    You are a Multi-Query RAG Generation Agent. Analyze the full conversation context to generate 2-3 focused search queries addressing the user's underlying information needs.\n\n    KEY APPROACH:\n    1. Analyze the COMPLETE conversation thread to identify the main topic being discussed\n    2. Generate queries about the actual information need, not communication issues\n\n    PRINCIPLES:\n    - Generate all queries in Korean language (한국어)\n    - Focus on substantive topics from conversation analysis\n    - Avoid queries about communication problems - target the underlying subject matter\n\n    Context:\n    - User's Latest Query: {{ user_query }}\n    - Conversation History: {{ conversation_history }}\n\n    Generate 2-3 Korean search queries that address the user's actual information need based on the conversation context.\n\n    Output Format:\n    {{ ctx.output_format }}\n    \"#\n}\n",
    "types/structs.baml": "class ConversationMessage {\n  role string\n  content string\n}\n\nclass MultiRAGQueryResult {\n  search_queries string[]\n}\n\n",
}

def get_baml_files():
    return _file_map